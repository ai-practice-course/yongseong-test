{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGUfT-SBmVTD",
        "outputId": "4ed6d245-c401-4675-8b72-fe600507a456"
      },
      "outputs": [],
      "source": [
        "# !pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4M4OxERWmcnY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import joblib\n",
        "import pickle\n",
        "import optuna\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 함수정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ChKBNVnrmcZC"
      },
      "outputs": [],
      "source": [
        "def sliding_windows(data, lookback_length, forecast_length):\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(lookback_length, len(data) - forecast_length + 1):\n",
        "        _x = data[(i-lookback_length) : i]\n",
        "        _y = data[i : (i + forecast_length)]\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "\n",
        "def get_data_loader(X, y, batch_size):\n",
        "\n",
        "    x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "    train_ds = TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train))\n",
        "    train_dl = DataLoader(train_ds, batch_size = batch_size)\n",
        "\n",
        "    val_ds = TensorDataset(torch.Tensor(x_val), torch.Tensor(y_val))\n",
        "    val_dl = DataLoader(val_ds, batch_size = batch_size)\n",
        "\n",
        "    input_size = x_train.shape[-1]\n",
        "\n",
        "    return train_dl, val_dl, input_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data loading && Preproces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tit7FldnXBk",
        "outputId": "229d313d-d218-4291-93fa-b4331d5698e6"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "data=pd.read_excel('./data_full.xlsx')\n",
        "data = data[data[\"Date\"].isin(pd.date_range('2016-01-04', '2019-11-30'))]\n",
        "pd.date_range('2016-01-04', '2020-01-31')\n",
        "pd.date_range('2016-01-04', '2020-03-31')\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data = data[['Date', 'REV OBD']]\n",
        "scale_cols = ['REV OBD']\n",
        "\n",
        "# Loockback_period & forecasting_period\n",
        "max_prediction_length = 20\n",
        "lookback_length = 60\n",
        "training_data_max = len(data) - max_prediction_length\n",
        "\n",
        "# 학습용 데이터\n",
        "data_p = data.iloc[:training_data_max, :]\n",
        "training_data = scaler.fit_transform(data_p[scale_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Metric 생성을 위한 oot sample 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SytPlWa-mchZ"
      },
      "outputs": [],
      "source": [
        "# max_prediction_length 만큼의 데이터는 예측 데이터와 비교를 위해 분리\n",
        "# Training set에 없는 데이터로 구성\n",
        "# Input과 output의 pair로 정의\n",
        "x_for_metric = scaler.fit_transform(data[training_data_max -lookback_length : training_data_max][scale_cols])\n",
        "y_for_metric = scaler.fit_transform(data[training_data_max:][scale_cols])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM은 1 step 뒤의 값만을 예측하므로, forecasting_period를 1로 두고 진행\n",
        "x, y = sliding_windows(training_data, lookback_length, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C9eplEkh3hvw"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
        "                            num_layers = num_layers, batch_first = True)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size  * num_layers, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size, device = x.device))\n",
        "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size, device = x.device))\n",
        "        \n",
        "        # Propagate input through LSTM\n",
        "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
        "        h_out = h_out.view(-1, self.hidden_size * self.num_layers)\n",
        "        out = self.fc(h_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3_IBkQhZ4CpP"
      },
      "outputs": [],
      "source": [
        "def train(log_interval, model, train_dl, val_dl, optimizer, criterion, epoch):\n",
        "\n",
        "    best_loss = np.inf\n",
        "    for epoch in range(epoch):\n",
        "        train_loss = 0.0\n",
        "        model.train()\n",
        "        for data, target in train_dl:\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "                model = model.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target) # mean-squared error for regression\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # validation\n",
        "        valid_loss = 0.0\n",
        "        model.eval()\n",
        "        for data, target in val_dl:\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            output = model(data)         \n",
        "            loss = criterion(output, target)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "        if ( epoch % log_interval == 0 ):\n",
        "            print(f'\\n Epoch {epoch} \\t Training Loss: {train_loss / len(train_dl)} \\t Validation Loss: {valid_loss / len(val_dl)} \\n')\n",
        "\n",
        "        if best_loss > (valid_loss / len(val_dl)):\n",
        "            print(f'Validation Loss Decreased({best_loss:.6f}--->{(valid_loss / len(val_dl)):.6f}) \\t Saving The Model')\n",
        "            best_loss = (valid_loss / len(val_dl))\n",
        "            torch.save(model.state_dict(), 'lstm_saved_model.pth')\n",
        "\n",
        "    return best_loss\n",
        "\n",
        "\n",
        "def smape(a, f):\n",
        "    return 1/len(a) * np.sum(2 * np.abs(f-a) / (np.abs(a) + np.abs(f))*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 60, 1)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aa = x_for_metric\n",
        "tmp = np.append( np.expand_dims(aa[1:, :], 0), np.expand_dims(y_for_metric[2, :], (0,2)), axis=1)\n",
        "tmp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "im6B6rzG4ClH"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "\n",
        "    cfg = { \n",
        "            'batch_size' : trial.suggest_categorical('batch_size',[64, 128, 256, 512]), # [64, 128, 256]\n",
        "            'learning_rate' : trial.suggest_loguniform('learning_rate', 1e-3, 1e-1), #trial.suggest_loguniform('learning_rate', 1e-2, 1e-1), # learning rate을 0.01-0.1까지 로그 uniform 분포로 사용\n",
        "            'hidden_size': trial.suggest_categorical('hidden_size',[16, 32, 64, 128, 256, 512, 1024]),\n",
        "            'num_layers': trial.suggest_int('num_layers', 1, 5, 1),       \n",
        "        }\n",
        "\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    log_interval = 5\n",
        "    num_classes = 1 # parameter에서 빼서 상수로 설정\n",
        "    num_epochs = 10 # parameter에서 빼서 상수로 설정\n",
        "\n",
        "    train_dl, val_dl, input_size = get_data_loader(x, y,  cfg['batch_size'])\n",
        "    \n",
        "    model = LSTM(\n",
        "        num_classes = num_classes, \n",
        "        input_size = input_size, \n",
        "        hidden_size = cfg['hidden_size'], \n",
        "        num_layers = cfg['num_layers']\n",
        "    )\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        \n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg['learning_rate'])\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    best_loss = train(log_interval, model, train_dl, val_dl, optimizer, criterion, num_epochs)\n",
        "\n",
        "    print('best loss for the trial = ', best_loss)\n",
        "    predict_data = []\n",
        "    # 여기서 x는 (sample, lookback_length, 1)의 크기를 지님. 따라서, 제일 앞의 시점을 제거하려면, x[:, -1, :]이 되어야 함\n",
        "    x_pred = np.expand_dims(x_for_metric, 0)  # Inference에 사용할 lookback data를 x_pred로 지정. 앞으로 x_pred를 하나씩 옮겨 가면서 inference를 할 예정\n",
        "\n",
        "    for j, i in enumerate(range(max_prediction_length)):\n",
        "\n",
        "        # feed the last forecast back to the model as an input\n",
        "        x_pred = np.append( x_pred[:, 1:, :], np.expand_dims(y_for_metric[j, :], (0,2)), axis=1)\n",
        "        xt_pred = torch.Tensor(x_pred)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            xt_pred = xt_pred.cuda()\n",
        "        # generate the next forecast\n",
        "        yt_pred = model(xt_pred)\n",
        "        # tensor to array\n",
        "        # x_pred = xt_pred.cpu().detach().numpy()\n",
        "        y_pred = yt_pred.cpu().detach().numpy()\n",
        "\n",
        "        # save the forecast\n",
        "        predict_data.append(y_pred)\n",
        "\n",
        "    # transform the forecasts back to the original scale\n",
        "    predict_data = np.array(predict_data).reshape(-1, 1)\n",
        "    SMAPE = SMAPE(y_for_metric, predict_data)\n",
        "    \n",
        "    print(f' \\nSMAPE : {SMAPE}')\n",
        "\n",
        "\n",
        "    return SMAPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDfe0XmG4ChP",
        "outputId": "0934ffae-e972-49be-d8fe-fbfcd9b842d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-06-18 16:02:23,616]\u001b[0m A new study created in memory with name: no-name-7461789a-55e8-4c33-a00b-a3d17ce22581\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 0 \t Training Loss: 0.11291885823011398 \t Validation Loss: 0.04117981716990471 \n",
            "\n",
            "Validation Loss Decreased(inf--->0.041180) \t Saving The Model\n",
            "Validation Loss Decreased(0.041180--->0.023098) \t Saving The Model\n",
            "Validation Loss Decreased(0.023098--->0.016731) \t Saving The Model\n",
            "\n",
            " Epoch 5 \t Training Loss: 0.5707897007465362 \t Validation Loss: 0.2192603498697281 \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-06-18 16:02:26,398]\u001b[0m Trial 0 finished with value: 58.02022040725072 and parameters: {'batch_size': 256, 'learning_rate': 0.008231698701227924, 'hidden_size': 256, 'num_layers': 4}. Best is trial 0 with value: 58.02022040725072.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best loss for the trial =  0.01673095254227519\n",
            " \n",
            "SMAPE : 58.02022040725072\n",
            "\n",
            " Epoch 0 \t Training Loss: 0.04637186957851929 \t Validation Loss: 0.0216861542314291 \n",
            "\n",
            "Validation Loss Decreased(inf--->0.021686) \t Saving The Model\n",
            "Validation Loss Decreased(0.021686--->0.020385) \t Saving The Model\n",
            "Validation Loss Decreased(0.020385--->0.019178) \t Saving The Model\n",
            "\n",
            " Epoch 5 \t Training Loss: 0.025722071966704202 \t Validation Loss: 0.019340337067842484 \n",
            "\n",
            "Validation Loss Decreased(0.019178--->0.019158) \t Saving The Model\n",
            "Validation Loss Decreased(0.019158--->0.019020) \t Saving The Model\n",
            "Validation Loss Decreased(0.019020--->0.018918) \t Saving The Model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-06-18 16:02:28,960]\u001b[0m Trial 1 finished with value: 65.06253435216556 and parameters: {'batch_size': 64, 'learning_rate': 0.0027539087138622894, 'hidden_size': 64, 'num_layers': 4}. Best is trial 0 with value: 58.02022040725072.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss Decreased(0.018918--->0.018815) \t Saving The Model\n",
            "best loss for the trial =  0.01881495025008917\n",
            " \n",
            "SMAPE : 65.06253435216556\n",
            "\n",
            " Epoch 0 \t Training Loss: 0.03477754878501097 \t Validation Loss: 0.0277174295236667 \n",
            "\n",
            "Validation Loss Decreased(inf--->0.027717) \t Saving The Model\n",
            "Validation Loss Decreased(0.027717--->0.019786) \t Saving The Model\n",
            "Validation Loss Decreased(0.019786--->0.016718) \t Saving The Model\n",
            "Validation Loss Decreased(0.016718--->0.016553) \t Saving The Model\n",
            "\n",
            " Epoch 5 \t Training Loss: 0.023597219958901405 \t Validation Loss: 0.016564274206757545 \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-06-18 16:02:29,891]\u001b[0m Trial 2 finished with value: 67.91946493770153 and parameters: {'batch_size': 128, 'learning_rate': 0.006623010612362953, 'hidden_size': 16, 'num_layers': 2}. Best is trial 0 with value: 58.02022040725072.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best loss for the trial =  0.01655278044442336\n",
            " \n",
            "SMAPE : 67.91946493770153\n",
            "\n",
            " Epoch 0 \t Training Loss: 1.9409560166737612 \t Validation Loss: 0.5459761917591095 \n",
            "\n",
            "Validation Loss Decreased(inf--->0.545976) \t Saving The Model\n",
            "Validation Loss Decreased(0.545976--->0.085423) \t Saving The Model\n",
            "Validation Loss Decreased(0.085423--->0.044357) \t Saving The Model\n",
            "Validation Loss Decreased(0.044357--->0.034689) \t Saving The Model\n",
            "\n",
            " Epoch 5 \t Training Loss: 0.033054652459481186 \t Validation Loss: 0.021773317083716392 \n",
            "\n",
            "Validation Loss Decreased(0.034689--->0.021773) \t Saving The Model\n",
            "Validation Loss Decreased(0.021773--->0.018333) \t Saving The Model\n",
            "Validation Loss Decreased(0.018333--->0.018072) \t Saving The Model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-06-18 16:02:32,219]\u001b[0m Trial 3 finished with value: 64.63681781193611 and parameters: {'batch_size': 64, 'learning_rate': 0.012339588600570424, 'hidden_size': 256, 'num_layers': 3}. Best is trial 0 with value: 58.02022040725072.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best loss for the trial =  0.01807173080742359\n",
            " \n",
            "SMAPE : 64.63681781193611\n",
            "\n",
            " Epoch 0 \t Training Loss: 0.07857944294810296 \t Validation Loss: 0.023710521403700113 \n",
            "\n",
            "Validation Loss Decreased(inf--->0.023711) \t Saving The Model\n",
            "Validation Loss Decreased(0.023711--->0.016537) \t Saving The Model\n",
            "Validation Loss Decreased(0.016537--->0.015468) \t Saving The Model\n",
            "\n",
            " Epoch 5 \t Training Loss: 0.024748937413096427 \t Validation Loss: 0.016025313176214695 \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-06-18 16:02:33,413]\u001b[0m Trial 4 finished with value: 68.26887900034627 and parameters: {'batch_size': 256, 'learning_rate': 0.008916999467114078, 'hidden_size': 128, 'num_layers': 4}. Best is trial 0 with value: 58.02022040725072.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss Decreased(0.015468--->0.015364) \t Saving The Model\n",
            "best loss for the trial =  0.015364142134785652\n",
            " \n",
            "SMAPE : 68.26887900034627\n"
          ]
        }
      ],
      "source": [
        "sampler = optuna.samplers.TPESampler()\n",
        "#   sampler = optuna.samplers.SkoptSampler()\n",
        "\n",
        "# model.load_state_dict(torch.load('lstm_saved_model.pth'))\n",
        "    \n",
        "study = optuna.create_study(sampler=sampler, direction='minimize')\n",
        "study.optimize(objective, n_trials= 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "LSTM-pytorch_optuna_05.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3adb55b36cf11984244f732b8044a5f95e16798e2d742560837f6dd41133e800"
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit ('image_crawler')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
